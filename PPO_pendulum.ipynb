{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma simples versão de PPO (Proximal Policy Optimization) baseada em:\n",
    "1. [https://arxiv.org/abs/1707.02286]\n",
    "2. [https://arxiv.org/abs/1707.06347]\n",
    "Veja mais nesses tutoriais: https://morvanzhou.github.io/tutorials\n",
    "e nesse video: https://www.youtube.com/watch?v=lehLSoMPmcM&t=144s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Importaçoes   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.compat.v1 as tf   # Workaround para retrocompatibilidade \n",
    "#tf.disable_v2_behavior()            # com tensorflow v1\n",
    "import tensorflow as tf\n",
    "import numpy as np                  # Numpy para trabalhar com arrays\n",
    "import matplotlib.pyplot as plt     # Matplotlib plota graficos matematicos\n",
    "import gym                          # GYM Environment: ambiente onde a simulação vai acontecer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Configurações   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_MAX = 600            # Qantidade total de episódios\n",
    "EP_LEN = 200            # Quantas sequencias vão acontecer dentro de cada episódio\n",
    "GAMMA = 0.9             # Avanço (?)\n",
    "A_LR = 0.0001           # Taxa de aprendizado do ATOR\n",
    "C_LR = 0.0002           # Taxa de aprendizado da CRITICA\n",
    "BATCH = 64              # Tamanho do pacote à entrar para treinamento em cada etapa (?)\n",
    "A_UPDATE_STEPS = 20     # Quantidade de vezes que o treinamento do ATOR vai tomar a cadeia de dados de batch\n",
    "C_UPDATE_STEPS = 20     # Quantidade de vezes que o treinamento da CRITICA vai tomar a cadeia de dados de batch\n",
    "S_DIM = 3               # S_DIM é a dimensao do estado, ou seja, quantas entradas ele terá\n",
    "A_DIM = 1               # A_DIM é a dimensão das ações, ou seja, quantas acões podem ser executadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = dict(\n",
    "    name='clip',    # Metodo de clip (Clipped surrogate objective) sujerido pelos papéis como mais eficiente\n",
    "    epsilon=0.2     # epsilon=0.2 Valor de epsilon sujerido pelos papéis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Implementaçao da classe ppo   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):  \n",
    "    # Classe PPO agrega:\n",
    "    #   As redes neurais ATOR e CRITICA;\n",
    "    #   Função para atualizar as redes neurais;\n",
    "    #   Função para obter o valor de aprendizagem da CRITICA;\n",
    "    #   Função para treinar as redes neurais;\n",
    "    #   Função para escolher uma ação;\n",
    "\n",
    "    def __init__(self): # Construtor da Classe\n",
    "        self.sess = tf.Session()    #inicializar uma seção do TensorFlow\n",
    "        # Declaração das entradas das redes:\n",
    "        self.tfs = tf.placeholder(  # Estado do ambiente: a rede recebe o estado do ambiente através desse placeholder\n",
    "            tf.float32,             #   Tipo do placeholder\n",
    "            [None, S_DIM],         #   Dimensoes do placeholder\n",
    "            'state'                 #   Nome do placeholder\n",
    "        )  \n",
    "\n",
    "        self.tfa = tf.placeholder(  # Ação escolhida pela rede é informada através desse placeholder \n",
    "            tf.float32,                 #   Tipo do placeholder\n",
    "            [None, A_DIM],              #   Dimensoes do placeholder\n",
    "            'action'                    #   Nome do placeholder\n",
    "        )  \n",
    "\n",
    "        self.tfadv = tf.placeholder(    # Calculo do ganho que a rede obteve no episódio, calculado fora da classe PPO.\n",
    "            tf.float32,                 #   Tipo do placeholder\n",
    "            [None, 1],                  #   Tamanho do placeholder\n",
    "            'advantage'                 #   Nome do placeholder\n",
    "        )                               # Esse placeholder é usado para treinar tanto o ATOR quanto a CRITICA\n",
    "\n",
    "        # CRITICA:\n",
    "        with tf.variable_scope('critic'):   \n",
    "            # Criação da rede neural:\n",
    "            l1 = tf.layers.dense(       # Camada 1 entrada da Critica: \n",
    "                self.tfs,               #   self.tfs é o placeholder do estado, funciona como entrada da rede\n",
    "                100,                    #   100 é o numero de neuronios \n",
    "                tf.nn.relu,             #   Relu é o tipo de ativação da saida da camada\n",
    "                name='layer1-critic'    #   name é o nome da camada\n",
    "            )   \n",
    "\n",
    "            self.v = tf.layers.dense(   # Camada de saida de valores da CRITICA: \n",
    "                l1,                     #   l1 é a variavel referente a primeira camada da rede, \n",
    "                1,                      #   1 é a quantidade de saidas da rede\n",
    "                name = 'V_layer'        #   name é o nome da camada  \n",
    "            )                           #   A saida dessa rede será o Q-Value, o status do progreço do aprendizado\n",
    "\n",
    "        # Metodo de treinamento para o CRITICA, ou seja, o metodo de aprendizagem:\n",
    "        with tf.variable_scope('ctrain'):\n",
    "            self.tfdc_r = tf.placeholder(   # A recompensa de cada episódio é inserida na rede através desse placeholder\n",
    "                tf.float32,                 #   Tipo do placeholder\n",
    "                [None, 1],                  #   Dimensoes do placeholder\n",
    "                'discounted_r'              #   Nome do placeholder\n",
    "            )     \n",
    "            self.advantage = self.tfdc_r - self.v   # Atraves da recompensa discounted_r/tfdc_r subtraida pelo\n",
    "                                                    # valor de aprendizagem V_layer/v obtemos a vantagem\n",
    "            self.closs = tf.reduce_mean(    # tf.reduce_mean calcula a média. \n",
    "                tf.square(                  # tf.square calcula o quadrado\n",
    "                    self.advantage          # da vantagem\n",
    "                )\n",
    "            )                               # ! Através disso obtemos em closs o Loss ou a Perda da CRITICA                  \n",
    "                                               \n",
    "            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)  # Ultilizamos o otimizador ADAM, com a taxa de aprendizado da CRITICA C_LR\n",
    "                                                                                # com a funçao minimize processamos os gradientes da CRITICA através da perda da CRITICA em closs\n",
    "                                                                                #   Poderiamos usar tambem o SGD como otimizador.\n",
    "        # ATOR:\n",
    "        #   Politica atual\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)                  # Criação da rede neural (pi) para a politica atual do ATOR através da função build_anet, definindo como treinavel\n",
    "                                                                                #   pi é a saida da rede e pi_params são os pesos (estado atual) da rede\n",
    "                                                                                #   Os pesos pi_params sao ultilizados para atualizar as politicas atual a antiga.\n",
    "\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)               # Tira uma amostra de açao da politica atual pi do ATOR\n",
    "\n",
    "        #   Politica antiga\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)    # Criação da rede neural oldpi para a politica antiga do ATOR através da função build_anet, definindo como não treinavel\n",
    "\n",
    "        with tf.variable_scope('update_oldpi'):                                                 # Atualização dos pesos dos pesos de oldpi tendo como referencia os pesos de pi\n",
    "            self.update_oldpi_op=[]\n",
    "            for p, oldp in zip(pi_params, oldpi_params):\n",
    "                self.update_oldpi_op.append(oldp.assign(p)) # A cada atualização da rede, os pesos da politica atual passam para a politica antiga\n",
    "                                                                                                # Update_oldpi_op acumula todos os valores de pi ao decorrer do episodio\n",
    "\n",
    "        # Implementação da função de perda PPO\n",
    "        with tf.variable_scope('loss'): # Funçao de perda:\n",
    "            with tf.variable_scope('surrogate_pp'):\n",
    "                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)    # O Ratio é a razão da probabilidade da ação tfa na politica nova \n",
    "                                                                    # pela probabilidade da ação tfa na politica antiga.\n",
    "                surr = ratio * self.tfadv                           # Surrogate é a Razão multiplicada pela vantagem\n",
    "\n",
    "            self.aloss = -tf.reduce_mean(                           # tf.educe_mean calcula a negativa da média do\n",
    "                tf.minimum(                                         #   menor valor entre\n",
    "                    surr,                                           #       o Surrogate e\n",
    "                    self.tfadv*                                     #       a multiplicação da vantagem\n",
    "                        tf.clip_by_value(                           #           pelo ratio clipado (limitado) por\n",
    "                            ratio,                                  #               \n",
    "                            1.-METHOD['epsilon'],                   #                no maximo 1 - o metodo Clipped surrogate objective\n",
    "                            1.+METHOD['epsilon']                    #                no minimo 1 + o metodo Clipped surrogate objective              \n",
    "                        )                                           # \n",
    "                )                                                   # Obtendo assim em aloss a perda do Ator\n",
    "            )\n",
    "\n",
    "        # Metodo de treinamento para o ATOR, ou seja, o metodo de aprendizagem:\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)  # Ultilizamos o otimizador ADAM, com a taxa de aprendizado do ATOR A_LR\n",
    "                                                                                # com minimize processamos os gradientes do ATOR através da perda do ATOR em aloss\n",
    "\n",
    "        tf.summary.FileWriter(\"log/\", self.sess.graph)      # Salvando o modelo na pasta log para analize futura no tensorboard\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())    # Inicializando todas as váriaveis definidas\n",
    "    \n",
    "    # Função de atualizaçao\n",
    "    def update(self, s, a, r):              # Recebe o estado, a ação e a recompensa\n",
    "        self.sess.run(self.update_oldpi_op) # Executa a matriz update_oldpi_op que comtem todos os pesos de pi/oldpi\n",
    "        \n",
    "        # Atualiza o ATOR\n",
    "        adv = self.sess.run(self.advantage, { self.tfs: s, self.tfdc_r: r })\n",
    "\n",
    "        for _ in range(A_UPDATE_STEPS):\n",
    "            self.sess.run(self.atrain_op, { self.tfs: s, self.tfa: a, self.tfadv: adv })\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            self.sess.run(self.ctrain_op, { self.tfs: s, self.tfdc_r: r })\n",
    "                                                                                                        \n",
    "\n",
    "    def _build_anet(self, name, trainable): \n",
    "        # Constroi as redes neurais do ATOR\n",
    "        #    name é o nome da rede\n",
    "        #    trainable determina se a rede é treinavel ou nao\n",
    "        with tf.variable_scope(name):   \n",
    "            l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu, trainable=trainable)\n",
    "\n",
    "            #   Calcula a ação que vai ser tomada\n",
    "            mu = 2 * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable, name = 'mu_'+name)                           #   O resultado é multiplicado por 2 para se adequar ao ambiente, que trabalha com um range 2 e -2.\n",
    "            \n",
    "            #   Calcula o desvio padrão, o range onde estará a possibilidade de ação    \n",
    "            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable, name ='sigma_'+name)    \n",
    "\n",
    "            polyce = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)   \n",
    "        return polyce, params    # Retorna a ação e os pesos atuais das redes para serem armazenados na politica antiga.\n",
    "\n",
    "    def choose_action(self, s):     # Recebe o estado s e retorna uma ação a\n",
    "        s = s[np.newaxis, :]        #   Recebe o estado s e \n",
    "        a = self.sess.run(self.sample_op,{self.tfs: s})[0]\n",
    "        return np.clip(a, -2, 2)    #   Retorna um valor de ação a clipado entre -2 e 2\n",
    "\n",
    "    def get_v(self, s):             # Recebe o estado s e retorna o valor da taxa de aprendizagem da CRITICA\n",
    "        if s.ndim < 2: s = s[np.newaxis, :] # \n",
    "        return self.sess.run(self.v, {self.tfs: s})[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Implementaçao do ambiente   #\n",
    "env = gym.make('Pendulum-v0').unwrapped # Instancia o ambiente pendulo\n",
    "ppo = PPO()                             # Instancia a classe PPO\n",
    "all_ep_r = []                           # Cria um array para a recompensa de todos os episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Loop de episódios   #\n",
    "for ep in range(EP_MAX):    # EP_MAX: quantidade de episodios \n",
    "    s = env.reset()         # Redefine o ambiente e armazena o estado atual em s\n",
    "    # Cria tres arrais para o episódio:\n",
    "    buffer_s = []   # buffer_s: buffer do estado\n",
    "    buffer_a = []   # buffer_a: buffer da ação\n",
    "    buffer_r = []   # buffer_r: buffer da recompensa         \n",
    "    ep_r = 0        # Recompensa do episódio\n",
    "#   Loop de episódio    #    \n",
    "    for t in range(EP_LEN):             # Duração de cada episodio\n",
    "        env.render()                    # Renderiza o ambiente\n",
    "        a = ppo.choose_action(s)        # Envia um estado s e recebe uma açao a \n",
    "        s_, r, done, _ = env.step(a)    # Envia uma açao a ao ambiente e recebe o estado s_, e a recompensa r\n",
    "        buffer_s.append(s)              # Adiciona ao buffer de estado o estado atual s\n",
    "        buffer_a.append(a)              # Adiciona ao buffer de ação a açao atual a\n",
    "        buffer_r.append((r+8)/8)        # Adiciona ao buffer de recompensa a recompensa atual (?) normalizada (r+8)/8\n",
    "        s = s_                          # Atualiza a variavel de estado com o estado recebido pelo ambiente\n",
    "        ep_r += r                       # soma a recompensa da ação a recompensa do episodio\n",
    "\n",
    "        # Atualiza PPO\n",
    "        if (t+1) % BATCH == 0 or t == EP_LEN-1: #\n",
    "            v_s_ = ppo.get_v(s_)                # Passa o estado atual s_ e recebe o valor atual da taxa de aprendizagem da CRITICA\n",
    "                                                # Obteniendo la respuesta de la NN del Critic, entregando el estado 's_' \n",
    "                                                # V = learned state-value function\n",
    "            discounted_r = []                   # Cria um array pra armazenar as recompensas calculadas\n",
    "            for r in buffer_r[::-1]: # [::-1] coloca ao contrario\n",
    "                v_s_ = r + GAMMA * v_s_         # Calcula a recompensa multiplicando a recompensa recebida r pela GAMMA \n",
    "                                                # e pelo valor da taxa de aprendizado do estado v_s_\n",
    "                discounted_r.append(v_s_)       # Adiciona ao array de recompensas calculadas \n",
    "            discounted_r.reverse()              # Coloca o array de recompensas calculadas ao contrario\n",
    "            # vstack trnasforma os arrays que estão em linha, em colunas\n",
    "            # Esses arrays de colunas sao armazenados em bs ba e br\n",
    "            bs = np.vstack(buffer_s) \n",
    "            ba = np.vstack(buffer_a)\n",
    "            br = np.array(discounted_r)[:, np.newaxis]\n",
    "            # Esvazia os buffers de estado, açao e recompensa\n",
    "            buffer_s = [] \n",
    "            buffer_a = []\n",
    "            buffer_r = []   \n",
    "            # Treine o cliente e o ator (status, ações, desconto de r)\n",
    "            ppo.update( # Atualiza as redes com:\n",
    "                bs,     #   Os estados aculmulados\n",
    "                ba,     #   As ações aculmuladas\n",
    "                br      #   As recompensas aculmuladas\n",
    "            )                      \n",
    "    # Adiciona a recompensa do episodio atual ao array de recompensas\n",
    "    if ep == 0: all_ep_r.append(ep_r) \n",
    "    else: all_ep_r.append(all_ep_r[-1]*0.9 + ep_r*0.1)\n",
    "    # Escreve na tela\n",
    "    print(\n",
    "        'Ep: %i' % ep,      # Numero do episodio\n",
    "        \"|Ep_r: %i\" % ep_r, # Recompensa do episodio\n",
    "        (\"|Lam: %.4f\" % METHOD['lam']) if METHOD['name'] == 'kl_pen' else '',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( # Plota o grafico de todas as recompensas\n",
    "    np.arange(\n",
    "        len(all_ep_r)\n",
    "    ), \n",
    "    all_ep_r\n",
    ")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Moving averaged episode reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
